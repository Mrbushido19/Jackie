"""
Agent LangChain avec outils Microsoft Graph et gestion du contexte
"""
from typing import Union
import re

from langchain.agents import Tool, AgentExecutor, create_react_agent
from langchain.prompts.base import BasePromptTemplate
from langchain.memory import ConversationBufferMemory
from langchain.chains import LLMChain
from langchain.agents import AgentOutputParser
from langchain.schema import AgentAction, AgentFinish


from tools.graph_tools import GraphTools
from llm.local_llm import LocalLLM

# ğŸ§  Template du prompt pour l'agent
# AGENT_PROMPT_TEMPLATE = """
# Tu es un assistant IA pour un collaborateur d'entreprise.
# Tu peux lire ses mails, son agenda et ses tÃ¢ches en utilisant des outils spÃ©cifiques {tools}

# Reponds aux questions en utilisant les outils microsoft Graph API disponibles.
# Tu as accÃ¨s aux outils suivants comme des actions que tu peux appeler: {tool_names}

# Question : {input}
# {agent_scratchpad}

# RÃ©ponds en utilisant ce format :
# Action: [nom de l'outil]
# Arguments: [arguments de l'outil]

# Sinon, si tu peux rÃ©pondre directement :
# Final Answer: [rÃ©ponse finale]
# """

class CustomOutputParser(AgentOutputParser):
    def parse(self, text: str) -> Union[AgentAction, AgentFinish]:
        """
        Analyse la sortie brute du LLM et en extrait l'action ou la rÃ©ponse finale.
        """
        if "Final Answer:" in text:
            return AgentFinish(
                return_values={"output": text.split("Final Answer:")[-1].strip()},
                log=text,
            )

        # Match l'action et les arguments
        match = re.search(r"Action:\s*(.*?)\s*Arguments:\s*(.*)", text, re.DOTALL)
        if not match:
            return AgentFinish(
                return_values={"output": "Je ne comprends pas la requÃªte."},
                log=text,
            )

        return AgentAction(
            tool=match.group(1).strip(),
            tool_input=match.group(2).strip(),
            log=text,
        )

class AssistantAgent:
    def __init__(self, graph_tools: GraphTools):
        self.llm = LocalLLM().llm  # Assure-toi que LocalLLM expose .llm
        self.memory = ConversationBufferMemory(memory_key="chat_history", return_messages=False)
        self.tools = self._build_tools(graph_tools)
        self.agent = self._build_agent()

    def _build_tools(self, graph_tools: GraphTools) -> list[Tool]:
        return [
            Tool(name="get_user_info", func=graph_tools.get_user_info, description="RÃ©cupÃ¨re les infos de l'utilisateur"),
            Tool(name="get_emails", func=graph_tools.get_emails, description="RÃ©cupÃ¨re les derniers emails"),
            Tool(name="get_events", func=graph_tools.get_events, description="RÃ©cupÃ¨re les prochains Ã©vÃ©nements"),
            Tool(name="get_tasks", func=graph_tools.get_tasks, description="RÃ©cupÃ¨re les tÃ¢ches Ã  venir"),
        ]

    def _build_agent(self) -> AgentExecutor:
        prompt = BasePromptTemplate(
            template="""Tu es un assistant IA qui aide Ã  gÃ©rer les informations personnelles et professionnelles.

            Tu as accÃ¨s aux outils suivants :
            {tools}

            Utilise ces outils pour rÃ©pondre aux questions de l'utilisateur.

            Historique de la conversation :
            {chat_history}

            Question de l'utilisateur : {input}

            RÃ©ponds en utilisant ce format :
            Action: [nom de l'outil]
            Arguments: [arguments de l'outil]

            Sinon, si tu peux rÃ©pondre directement :
            Final Answer: [rÃ©ponse finale]
                """,
            input_variables=["input", "chat_history", "tools"]
        )
        
        llm_chain = LLMChain(llm=self.llm, prompt=prompt)
        output_parser = CustomOutputParser()

        agent = create_react_agent(
            llm=llm_chain,
            prompt=prompt,
            output_parser=output_parser,
            tools=self.tools,
        )

        return AgentExecutor.from_agent_and_tools(
            agent=agent,
            tools=self.tools,
            memory=self.memory,
            verbose=True
        )

    def process_query(self, query: str) -> str:
        """
        Traite une requÃªte utilisateur via l'agent IA.
        """
        try:
            return self.agent.invoke({"input": query})["output"]
        except Exception as e:
            return f"Erreur lors du traitement : {str(e)}"
